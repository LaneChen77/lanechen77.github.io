---
permalink: /projects/
title: "Research Projects"
excerpt: "Research projects and contributions"
author_profile: true  
---

<span class='anchor' id='research-projects'></span>

# üöÄ Featured Research Projects

## BEVVLM - BEV Vision Language Model

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Main Project</div><img src='images/500x300.png' alt="BEVVLM Project" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**BEVVLM: Integrating Bird's Eye View with Vision Language Models**](https://github.com/LaneChen77/BEVVLM)

**Description:** BEVVLM represents a groundbreaking approach to autonomous driving perception by combining the spatial advantages of Bird's Eye View (BEV) representation with the semantic understanding capabilities of Vision Language Models (VLMs). This project addresses critical challenges in multi-modal scene understanding for autonomous vehicles.

**Key Technical Achievements:**
- Novel multi-modal fusion architecture combining BEV spatial representation with natural language processing
- Real-time inference pipeline optimized for autonomous driving applications  
- Comprehensive evaluation framework on standard autonomous driving benchmarks
- Integration of 3D object detection with semantic scene understanding
- Advanced attention mechanisms for cross-modal feature alignment

**Technical Stack:**
- **Framework:** PyTorch, MMCV, MMDetection3D
- **Languages:** Python, CUDA
- **Tools:** ROS, Docker, Weights & Biases
- **Hardware:** GPU cluster with multi-camera setups

**Impact & Applications:**
- Enhanced scene understanding for autonomous vehicles
- Improved safety through better contextual awareness
- Applications in traffic scene analysis and prediction
- Potential for deployment in real-world autonomous driving systems

[**üîó GitHub Repository**](https://github.com/LaneChen77/BEVVLM) | [**üìñ Documentation**](https://github.com/LaneChen77/BEVVLM#readme) | [**üìä Results**](https://github.com/LaneChen77/BEVVLM#results)

</div>
</div>

---

## Upcoming Projects

### Multi-Modal 3D Object Detection
**Status:** In Development  
**Focus:** Advanced 3D object detection using multi-camera inputs and radar/LiDAR fusion for robust autonomous driving perception.

**Planned Features:**
- Multi-sensor fusion architecture
- Real-time 3D object detection and tracking
- Weather and lighting condition robustness
- Integration with BEVVLM framework

### Autonomous Driving Scene Understanding
**Status:** Research Phase  
**Focus:** Developing comprehensive scene understanding capabilities that combine visual perception with contextual reasoning.

**Research Goals:**
- Semantic scene graph generation
- Temporal reasoning for dynamic scenes
- Interaction prediction between traffic participants
- Integration with natural language descriptions

### Vision-Language Navigation
**Status:** Conceptual  
**Focus:** Exploring the intersection of computer vision and natural language processing for autonomous navigation tasks.

**Research Direction:**
- Natural language instruction following
- Visual reasoning for navigation
- Multi-modal representation learning
- Human-robot interaction in driving contexts

---

# üõ†Ô∏è Open Source Contributions

## Academic Community Involvement
- **Code Sharing:** All research code made publicly available on GitHub
- **Documentation:** Comprehensive documentation and tutorials for reproducibility
- **Community Support:** Active engagement with the computer vision and autonomous driving research communities

## Collaborative Research
- **Research Group Projects:** Contributing to collaborative projects within the computer vision lab
- **Cross-Institutional Collaboration:** Working with industry partners on real-world applications
- **Open Datasets:** Contributing to and utilizing open autonomous driving datasets

---

# üìà Project Impact & Metrics

- **GitHub Stars:** Tracking community engagement and adoption
- **Research Citations:** Academic impact through peer-reviewed publications
- **Code Contributions:** Regular commits and feature developments
- **Community Engagement:** Active participation in research forums and conferences

---

# üéØ Future Research Directions

The current and upcoming projects are designed to push the boundaries of what's possible in autonomous driving perception:

1. **Enhanced Multi-Modal Understanding:** Developing more sophisticated fusion techniques
2. **Real-World Deployment:** Transitioning research prototypes to production-ready systems
3. **Ethical AI:** Ensuring fairness and safety in autonomous driving systems
4. **Scalability:** Creating solutions that work across diverse geographic and cultural contexts

*This page will be updated regularly as projects evolve and new research initiatives begin.*